{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM and Sequence Generation\n",
    "\n",
    "In this notebook we're going to explore two changes to RNN we built in the previous notebook. \n",
    "\n",
    "1. The use of LSTM layers instead of SimpleRNN layers.\n",
    "2. Generating sequences instead of generating a single prediction as output.\n",
    "\n",
    "## LSTM\n",
    "\n",
    "A Long Short Term Memory layer is an extension of the RNN idea, and one that is designed to do 2 things:\n",
    "\n",
    "1. Give the hidden state more flexibillity in how it updates.\n",
    "2. Provide an efficent route for backpropagation, similar to skip layers in CNNs\n",
    "\n",
    "Here is a diagram from one of the readings [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/):\n",
    "\n",
    "![](https://colah.github.io/images/post-covers/lstm.png)\n",
    "\n",
    "What you see here are the addition of several \"gates\" as well as an additional output from the layer compared to a simple RNN. Gates act like other hidden layers and have their own learned weights. \n",
    "\n",
    "The two states are called \"cell state\" (the top line) and \"hidden state\" the bottom line. Both serve a similar purpose to the RNN hidden state, but the cell state's progression doesn't involve an activation function over time which helps avoid the vanishing gradient problem that simple RNN's often suffer from.\n",
    "\n",
    "The gates are typically referred to as follows:\n",
    "\n",
    "**Forget gate**: The first gate uses a sigmoid activation to produce values between 0 and 1, those values are then pointwise multiplied with the incoming hidden state. This allows the cell state to \"forget\" irrelevant context when the activations are near 0. This gate is designed such that it can move the values in the cell state closer to zero.\n",
    "\n",
    "**Input and \"gate\" gates**: The second gate is a multiply and involves the sigmoid and tanh activations. The multiply gate allows the sigmoid to scales the tanh output. The result is a value between -1 and 1 which gets pointwise added to the cell state (after the forget is applied). This allows the cell state to incrementally learn new information and store it into the context. \n",
    "\n",
    "The sigmoid is called the \"input\" gate and the tanh is called the \"gate gate\" but they both work together to decide how much the cell state learns from the new input.\n",
    "\n",
    "**Output gate**: Finally, the last sigmoid is the output gate, and it scales the output next output and hidden state values, but not the cell state.\n",
    "\n",
    "## Sequence to sequence\n",
    "\n",
    "The next change we'll make is to allow multiple outputs over time, allowing us to generate text output from text input rather than a single output for classification or regression.\n",
    "\n",
    "![](https://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "This requires a few changes.\n",
    "\n",
    "1. First, the training data. We'll be using a setup where our inputs and labels will be from the exact same text, but at every timestep the label will be the word directly following the current word. \n",
    "2. Second we have to enable each timestep to produce a prediction.\n",
    "3. We have to map those numeric predictions to a word.\n",
    "4. We have to enable the network to stop somehow, we'll be allowing the network to generate as one of the \"words\" a \"stop\" token which when predicted will cause the network to stop.\n",
    "\n",
    "Note that this strategy works for generating text both character by character and word by word. We will be performing a character by character training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[20 49 58 59 60  3 17 49 60 49 66 45 54 12  2 16 45 46 55 58 45  3 63 45\n",
      "  3 56 58 55 43 45 45 44  3 41 54 65  3 46 61 58 60 48 45 58  8  3 48 45\n",
      " 41 58  3 53 45  3 59 56 45 41 51 10  2  2 15 52 52 12  2 33 56 45 41 51\n",
      "  8  3 59 56 45 41 51 10  2  2 20 49 58 59 60  3 17 49 60 49 66 45 54 12\n",
      "  2 39 55 61], shape=(100,), dtype=int64) \n",
      " b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "\n",
      "tf.Tensor(\n",
      "[49 58 59 60  3 17 49 60 49 66 45 54 12  2 16 45 46 55 58 45  3 63 45  3\n",
      " 56 58 55 43 45 45 44  3 41 54 65  3 46 61 58 60 48 45 58  8  3 48 45 41\n",
      " 58  3 53 45  3 59 56 45 41 51 10  2  2 15 52 52 12  2 33 56 45 41 51  8\n",
      "  3 59 56 45 41 51 10  2  2 20 49 58 59 60  3 17 49 60 49 66 45 54 12  2\n",
      " 39 55 61  3], shape=(100,), dtype=int64) \n",
      " b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# load the tiny_shakespear dataset, 40,000 lines of text from various Shakspear plays\n",
    "# but only the \"train\" subset.\n",
    "dataset = tfds.load(name='tiny_shakespeare')['train']\n",
    "\n",
    "# Split the dataset from each line being a string to each line being an array of characters\n",
    "# in UTF-8 encoding\n",
    "dataset = dataset.map(lambda x: tf.strings.unicode_split(x['text'], 'UTF-8'))\n",
    "\n",
    "# Extract all the uniqe charcaters to form the vocabulary\n",
    "vocabulary = sorted(set(next(iter(dataset)).numpy()))\n",
    "\n",
    "# We're creating two functions to swap between characters and their int lookup value\n",
    "ids_from_chars = StringLookup(\n",
    "    vocabulary=list(vocabulary)\n",
    ")\n",
    "\n",
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True\n",
    ")\n",
    "\n",
    "# For training at each step we're asking the model to predict the next character\n",
    "# based on the current state + current character.\n",
    "dataset = dataset.map(lambda x: (ids_from_chars(x[:-1]), ids_from_chars(x[1:])))\n",
    "\n",
    "# Unbatch kind of flattens the data.\n",
    "# We're sort of implying that any line can flow fluidly into another line\n",
    "dataset = dataset.unbatch()\n",
    "\n",
    "# Now we're chopping the flat data into sequences of 100 characters each\n",
    "seq_len = 100\n",
    "dataset = dataset.batch(seq_len, drop_remainder = True)\n",
    "\n",
    "# We can see that \"next_char\" is just the \"cur_char\" shifted one position.\n",
    "# Because they are already encoded as UTF code points, they have a resaonblly efficent \n",
    "# numeric representation.\n",
    "for current_char, next_char in dataset.take(1):\n",
    "    print(current_char, '\\n', b''.join(chars_from_ids(current_char).numpy()))\n",
    "    print()\n",
    "    print(next_char, '\\n', b''.join(chars_from_ids(next_char).numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we're going to shuffle and batch it. Standard when working with Tensorflow's Dataset class\n",
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "Some things to note: \n",
    "\n",
    "1. We're using an LSTM rather than a simple RNN layer.\n",
    "2. It has a larger relative internal representation.\n",
    "3. The embedding dimension is similarly larger than last time.\n",
    "4. The output dimension is the size of the vocab (every unique character in the training data!)\n",
    "\n",
    "Also, we're actually subclassing the model class. This allows us to have a bit more control over when and how the layers talk to each other, and how we deal with state. Code slightly adapted from TF docs [https://www.tensorflow.org/tutorials/text/text_generation](https://www.tensorflow.org/tutorials/text/text_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 3 51 49 ... 54 47  3]\n",
      " [56 59  8 ...  3 44 45]\n",
      " [ 3 47 45 ... 48 41 52]\n",
      " ...\n",
      " [45 58 45 ...  3 41 54]\n",
      " [45 48 55 ... 44  3 41]\n",
      " [55 44 61 ... 54  3 43]], shape=(64, 100), dtype=int64)\n",
      "(64, 100, 67) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"text_generator_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  17152     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  68675     \n",
      "=================================================================\n",
      "Total params: 5,332,803\n",
      "Trainable params: 5,332,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class TextGeneratorModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM Second\n",
    "        self.lstm = tf.keras.layers.LSTM(rnn_units,\n",
    "                                       return_sequences=True, \n",
    "                                       return_state=True)\n",
    "\n",
    "        # Dense last.\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, h_states=None, return_state=False, training=False):\n",
    "        # Transform the input using the embedding layer\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "\n",
    "        # If there is no incoming state from an earlier timestep\n",
    "        # use the default initial state behavior\n",
    "        if h_states is None:\n",
    "            h_states = self.lstm.get_initial_state(x)\n",
    "        \n",
    "        # Transfrom the embedding using lstm\n",
    "        # This is transformed output, transformed hidden state, and _ is hidden cell state\n",
    "        x, h_states, c_states = self.lstm(x, initial_state=h_states, training=training)\n",
    "\n",
    "        # Pass the transformed x into the dense layer\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        # Only return state when asked\n",
    "        if return_state:\n",
    "            return x, h_states, c_states\n",
    "        else: \n",
    "            return x\n",
    "\n",
    "\n",
    "# embedding output == 256, rnn units = 1024\n",
    "model = TextGeneratorModel(len(ids_from_chars.get_vocabulary()), 256, 1024)\n",
    "\n",
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch)\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b' king to-day, my Lord of Derby?\\n\\nDERBY:\\nBut now the Duke of Buckingham and I\\nAre come from visiting '\n",
      "\n",
      "Predictions:\n",
      " b\"LP:D;e,feBOYXaqpMMUrG'?zHQAavq&P-AiAJRQxwBhPsvndd;'yU:qpCg!CR!vnw$he!tq DVAXm!&IsoQzR$REopSeM!BQIG\"\n"
     ]
    }
   ],
   "source": [
    "# Lets see what happens on the untrained network...\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "print(\"Input:\\n\", b''.join(chars_from_ids(input_example_batch[0]).numpy()))\n",
    "print()\n",
    "print(\"Predictions:\\n\", b''.join(chars_from_ids(sampled_indices).numpy()))\n",
    "# Of course... a lot of garbage :)\n",
    "# But that's just because it's not trained... hopefully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 67)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         4.2039866\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "156/156 [==============================] - 370s 2s/step - loss: 3.6661\n",
      "Epoch 2/20\n",
      "156/156 [==============================] - 334s 2s/step - loss: 3.1649\n",
      "Epoch 3/20\n",
      " 19/156 [==>...........................] - ETA: 4:37 - loss: 2.8563"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4)\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    dataset, \n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text With The Model\n",
    "\n",
    "Using a model to generate text is done by using the outputs from the dense layer as a sampling distribution. At each step we run the model once, maintaining the state, sampling from the prediction, and then using our sampled character as input for the next timestep. Tensorflow's documentation has a wonderful visual and class to help us do this more easily, so we've stolen both. From [https://www.tensorflow.org/tutorials/text/text_generation](https://www.tensorflow.org/tutorials/text/text_generation)\n",
    "\n",
    "![](https://www.tensorflow.org/tutorials/text/images/text_generation_sampling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature=temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices = skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits] \n",
    "        predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
    "                                              return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
